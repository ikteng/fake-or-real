{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":105874,"databundleVersionId":12964783,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-01T15:02:24.311918Z","iopub.execute_input":"2025-08-01T15:02:24.312287Z","iopub.status.idle":"2025-08-01T15:02:24.317646Z","shell.execute_reply.started":"2025-08-01T15:02:24.312258Z","shell.execute_reply":"2025-08-01T15:02:24.316533Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom torch.optim import AdamW\nfrom transformers import AutoModel, AutoTokenizer, get_linear_schedule_with_warmup\nfrom sklearn.model_selection import train_test_split\nfrom datasets import Dataset, DatasetDict\nfrom pathlib import Path\nfrom collections import defaultdict\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\nimport random","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T16:20:17.218762Z","iopub.execute_input":"2025-08-01T16:20:17.219283Z","execution_failed":"2025-08-02T02:44:53.999Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =========================================================\n# ‚úÖ 1. Data Augmentation: Word Dropout\n# =========================================================\n\ndef word_dropout(text, p=0.1):\n    words = text.split()\n    if len(words) <= 5:\n        return text\n    keep = [w for w in words if random.random() > p]\n    return \" \".join(keep) if keep else text","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =========================================================\n# ‚úÖ 2. Dataset Generators (with augmentation for training)\n# =========================================================\n\ntrain_dir = \"/kaggle/input/fake-or-real-the-impostor-hunt/data/train\"\ntest_dir  = \"/kaggle/input/fake-or-real-the-impostor-hunt/data/test\"\ntrain_csv = \"/kaggle/input/fake-or-real-the-impostor-hunt/data/train.csv\"\n\ndef train_generator(augment=True):\n    df = pd.read_csv(train_csv)\n    for _, row in df.iterrows():\n        folder = Path(train_dir) / f\"article_{row['id']:04d}\"\n        for text_id in [1, 2]:\n            file_path = folder / f\"file_{text_id}.txt\"\n            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n                text = f.read()\n            label = 1 if text_id == row[\"real_text_id\"] else 0\n\n            # Original sample\n            yield {\"id\": row[\"id\"], \"text\": text, \"text_id\": text_id, \"label\": label}\n\n            # Augmented sample (only for training)\n            if augment and random.random() < 0.5:\n                yield {\"id\": row[\"id\"], \"text\": word_dropout(text), \"text_id\": text_id, \"label\": label}\n\ndef test_generator():\n    data_dir = Path(test_dir)\n    folders = sorted([f for f in data_dir.iterdir() if f.is_dir()])\n    for folder in folders:\n        folder_id = int(folder.name.split(\"_\")[1])\n        for text_id in [1, 2]:\n            file_path = folder / f\"file_{text_id}.txt\"\n            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n                text = f.read()\n            yield {\"id\": folder_id, \"text\": text, \"text_id\": text_id}\n\n# Create datasets\ntrain_dataset = Dataset.from_generator(lambda: train_generator(augment=True))\ntest_dataset  = Dataset.from_generator(test_generator)\n\nraw_datasets = DatasetDict({\n    \"train\": train_dataset,\n    \"test\": test_dataset\n})\n\n# Split into train/val\ntrain_idx, val_idx = train_test_split(\n    range(len(raw_datasets['train'])), \n    test_size=0.2, \n    stratify=raw_datasets['train']['label'],\n    random_state=42\n)\ntrain_split = raw_datasets['train'].select(train_idx)\nval_split   = raw_datasets['train'].select(val_idx)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class SiameseSelfAttentionNetwork(nn.Module):\n    def __init__(self, model_name, num_labels=2, dropout=0.3):\n        super().__init__()\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.backbone = AutoModel.from_pretrained(model_name)\n        self.backbone.gradient_checkpointing_enable()\n        hidden_size = self.backbone.config.hidden_size\n\n        # Attention Layer\n        self.cross_attn = nn.MultiheadAttention(hidden_size, num_heads=self.backbone.config.num_attention_heads, batch_first=True)\n\n        # Interaction Head\n        self.interaction_head = nn.Sequential(\n            nn.Linear(hidden_size, hidden_size//2), nn.ReLU(), nn.Dropout(dropout),\n            nn.Linear(hidden_size//2, hidden_size//4), nn.ReLU(), nn.Dropout(dropout)\n        )\n\n        # Classifier\n        self.classifier = nn.Linear(hidden_size//4, num_labels)\n\n        # Tokenizer\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n\n    def extract_mean_pooling(self, texts):\n        encoded = self.tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(self.device)\n        outputs = self.backbone(**encoded)\n        last_hidden = outputs.last_hidden_state\n        mask = encoded['attention_mask'].unsqueeze(-1).expand(last_hidden.size())\n        mean_vec = (last_hidden * mask).sum(1) / mask.sum(1)\n        return mean_vec\n\n    def forward(self, texts, labels=None):\n        vecs = self.extract_mean_pooling(texts)\n        q = k = v = vecs.unsqueeze(1)\n        attn_out, _ = self.cross_attn(q, k, v)\n        features = self.interaction_head(attn_out.squeeze(1))\n        logits = self.classifier(features)\n\n        loss = None\n        if labels is not None:\n            criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n            loss = criterion(logits, labels)\n\n        return type('Out', (), {'loss': loss, 'logits': logits})()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_fn(model, dataloader, optimizer, scheduler):\n    model.train()\n    total_loss = 0\n    for batch in tqdm(dataloader, desc=\"Training\"):\n        texts, labels = batch['text'], batch['label'].to(device)\n        optimizer.zero_grad()\n        out = model(texts, labels)\n        out.loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step(); scheduler.step()\n        total_loss += out.loss.item()\n    return total_loss / len(dataloader)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def validate_fn(model, dataloader):\n    model.eval()\n    total_loss, preds, labels_all = 0, [], []\n    with torch.no_grad():\n        for batch in dataloader:\n            texts, labels = batch['text'], batch['label'].to(device)\n            out = model(texts, labels)\n            total_loss += out.loss.item()\n            pred = torch.argmax(out.logits, 1)\n            preds.extend(pred.cpu().numpy())\n            labels_all.extend(labels.cpu().numpy())\n    acc = (np.array(preds) == np.array(labels_all)).mean()\n    return total_loss / len(dataloader), acc","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =========================================================\n# ‚úÖ 5. Training Loop with Early Stopping + Gradual Unfreezing\n# =========================================================\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# model_name = \"distilbert-base-uncased\"\nmodel_name = \"roberta-base\"\nmodel = SiameseSelfAttentionNetwork(model_name).to(device)\n\n# Freeze backbone initially\nfor p in model.backbone.parameters():\n    p.requires_grad = False\n\n# Separate LRs: smaller for backbone\nparam_groups = [\n    {\"params\": model.backbone.parameters(), \"lr\": 1e-5},\n    {\"params\": [p for n,p in model.named_parameters() if \"backbone\" not in n], \"lr\": 3e-5}\n]\noptimizer = AdamW(param_groups, weight_decay=0.01)\n\ntrain_loader = DataLoader(train_split, batch_size=4, shuffle=True)\nval_loader   = DataLoader(val_split, batch_size=4, shuffle=False)\n\ntotal_steps = len(train_loader) * 20\nscheduler = get_linear_schedule_with_warmup(optimizer, 0, total_steps)\n\nbest_val_loss = float('inf')\nbest_metrics = {}  # <-- store train/val metrics\npatience, counter = 3, 0\n\nfor epoch in range(20):\n    if epoch == 3:  # Unfreeze backbone after 3 epochs\n        for p in model.backbone.parameters(): \n            p.requires_grad = True\n        optimizer = AdamW(param_groups, weight_decay=0.01)\n\n    train_loss = train_fn(model, train_loader, optimizer, scheduler)\n    val_loss, val_acc = validate_fn(model, val_loader)\n    print(f\"Epoch {epoch+1}: Train {train_loss:.4f} | Val {val_loss:.4f} | Acc {val_acc:.4f}\")\n\n    # ‚úÖ Save metrics when saving best model\n    if val_loss < best_val_loss - 1e-4:\n        best_val_loss = val_loss\n        best_metrics = {\n            \"epoch\": epoch + 1,\n            \"train_loss\": train_loss,\n            \"val_loss\": val_loss,\n            \"val_acc\": val_acc\n        }\n        torch.save(model.state_dict(), \"best_model.pth\")\n        counter = 0\n    else:\n        counter += 1\n        if counter >= patience:\n            print(\"‚èπÔ∏è Early stopping triggered\")\n            break\n\nprint(\"\\nüìå Best Model Metrics:\")\nprint(f\"Epoch {best_metrics['epoch']}: \"\n      f\"Train Loss={best_metrics['train_loss']:.4f}, \"\n      f\"Val Loss={best_metrics['val_loss']:.4f}, \"\n      f\"Val Accuracy={best_metrics['val_acc']:.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =========================================================\n# ‚úÖ 6. Prediction on Test Set\n# =========================================================\ndef predict_fn(model, dataloader):\n    model.eval()\n    probs_dict = defaultdict(dict)\n    with torch.no_grad():\n        for batch in tqdm(dataloader, desc=\"Predicting\"):\n            texts, ids, text_ids = batch[\"text\"], batch[\"id\"], batch[\"text_id\"]\n            out = model(texts)\n            probs = F.softmax(out.logits, 1)[:,1]\n            for a, t, p in zip(ids, text_ids, probs):\n                probs_dict[int(a)][int(t)] = p.item()\n    preds = {a: max(p.items(), key=lambda x:x[1])[0] for a,p in probs_dict.items()}\n    return preds\n\ntest_loader = DataLoader(raw_datasets['test'], batch_size=4)\npreds = predict_fn(model, test_loader)\n\n# Save submission\nsubmission = pd.DataFrame({\"id\": list(preds.keys()), \"real_text_id\": list(preds.values())})\nsubmission.to_csv(\"submission.csv\", index=False)\nprint(\"‚úÖ Saved submission.csv\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}